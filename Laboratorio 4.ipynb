{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e1c36d",
   "metadata": {},
   "source": [
    "# Laboratorio 4 Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20a13a",
   "metadata": {},
   "source": [
    "Task 1.1, leemos los archivos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9d1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Leer el archivo csv\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "x = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa8cc8",
   "metadata": {},
   "source": [
    "Task 1.3 - 1.4\n",
    "\n",
    "Implementamos el algoritmo de cross validation junto al algoritmo de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd663d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions =  np.dot(X,theta)\n",
    "    cost = np.sum((predictions-y)**2) / (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fd1db",
   "metadata": {},
   "source": [
    "Ahora, en orden para poder realizar el task 1.2 debemos definir la potencia con la que trabajaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef290a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of k values\n",
    "k_range = range(1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d48d2c",
   "metadata": {},
   "source": [
    "Task 1.2 pasamos a notacion matricial los datos de x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cd3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different k values\n",
    "for k in k_range:\n",
    "\n",
    "    X = []\n",
    "    for xi in x:\n",
    "        X.append([xi**g for g in range(0, k + 1)])\n",
    "    X = np.array(X)\n",
    "    X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26417dae",
   "metadata": {},
   "source": [
    "Definimos los folds del algoritmo de cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d162fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334ae30",
   "metadata": {},
   "source": [
    "Trabajamos con los folds y desarrollamos el algoritmo de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 10, loss = 423015083816.7794\n",
      "Best k value: 2\n"
     ]
    }
   ],
   "source": [
    "    loss = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "\n",
    "        # Dividir el dataset en data de entreno y test\n",
    "        X_train, y_train = X[train_idx], y[train_idx]  # variables de entreno\n",
    "        X_val, y_val = X[val_idx], y[val_idx]  # variables de testeo\n",
    "\n",
    "        # Normalizar los datos de entrada\n",
    "        X_train = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "        X_val = (X_val - np.mean(X_val)) / np.std(X_val)\n",
    "\n",
    "        # Definir los parámetros del modelo y la tasa de aprendizaje\n",
    "        theta = np.random.randn(1, k + 1)[0]  # pesos aleatorios\n",
    "        lr = 0.0001  # tasa de aprendizaje\n",
    "        n_iterations = 1000  # número máximo de iteraciones\n",
    "\n",
    "        # Ejecutar el descenso del gradiente para encontrar los coeficientes del modelo\n",
    "        for iteration in range(n_iterations):\n",
    "            gradient = (-2 * np.dot(X_train.T, y_train)) / len(\n",
    "                y_train\n",
    "            )  # obtener el gradiente\n",
    "            theta = theta - lr * gradient  #\n",
    "            cost = compute_cost(X_train, y_train, theta)\n",
    "\n",
    "        # Obtener la perdida y la almacenamos\n",
    "        L = (np.transpose((np.subtract(y_val, X_val.dot(theta))))).dot(\n",
    "            np.subtract(y_val, X_val.dot(theta))\n",
    "        )\n",
    "        L = L * (1 / len(X_val))\n",
    "        loss.append(L)\n",
    "\n",
    "    # Operar la media de todas las perdidas almacenadas\n",
    "    print(f\"k = {k}, loss = {np.mean(loss)}\")\n",
    "    # Encontrar el valor de k con la menor perdida promedio\n",
    "best_k = np.argmin(loss)\n",
    "print(\"Best k value:\", best_k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6a25daf",
   "metadata": {},
   "source": [
    "Para poder recorrer el Array no deja correrlo en Jupyter Notebook por alguna razón, entonces para ver la ejecución podemos correr el main.py para ver como recorre todos los valores del array y encuentra el mejor valor de k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2577f06e",
   "metadata": {},
   "source": [
    "Task 1.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "847f103f",
   "metadata": {},
   "source": [
    "Hallazgos y Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b341b5a8",
   "metadata": {},
   "source": [
    "- Debido a que la pérdida media del modelo en el conjunto de validación es un número bastante grande, el modelo no tiene el rendimiento previsto para encontrar los valores de salida.\n",
    "- Creemos que el modelo podria estar sufriendo overfitting o underfitting, lo que significa que el modelo puede estar complicandose al momento de capturar la verdadera relación entre las variables de entrada y salida en los datos de entrenamiento.\n",
    "- Al momento de aumentar el número de iteraciones descubrimos que puede ayudar al modelo a converger a una solución mejor.\n",
    "- Concluimos que la normalización de los datos de entrada es una buena práctica, ya que ayuda al modelo a converger más rápido y mejora su rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
